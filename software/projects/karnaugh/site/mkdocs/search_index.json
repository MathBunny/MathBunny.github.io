{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome\n\n\nWelcome to the Karnaugh Map Simplifier software documentation site. The purpose of this website is to provide documentation for both developers and to users. This includes theoretical material on approaches to simplifying boolean algebra expressions both by hand and through computation.\n\n\nHistory and Objective\n\n\nThe purpose of this software is to provide a collection of tools for engineers to simplify logical expressions and to visualize the results in a Karnaugh Map. There are two versions of the software. The first version, which was started in 2015 used Java Swing and supported up to four variable visual expression simplification. In the summer of 2017, a new version started development which is being made in JavaFX. \n\n\nFuture Goals\n\n\nThe ultimate goal for this project is to become the standard industry tool in working with boolean algebra expressions. This includes simplification of large expressions, and working on converting expressions between different forms.\n\n\nDownload Links\n\n\n\n\n\n\n\n\nDescription\n\n\nJava Required\n\n\nLink\n\n\n\n\n\n\n\n\n\n\nLatest stable build\n\n\nJRE 1.6\n\n\nDownload\n\n\n\n\n\n\nLatest stable source\n\n\nJDK 1.7\n\n\nDownload\n\n\n\n\n\n\nDevelopment source\n\n\nJDK 1.8\n\n\nGitHub\n\n\n\n\n\n\n\n\nYouTube Tutorials \n Demos\n\n\n\n\n\n\n\n\nTutorial Topic\n\n\nYouTube Link\n\n\n\n\n\n\n\n\n\n\nGeneral Software Demonstration\n\n\nView video\n\n\n\n\n\n\n\n\nJavaDoc Documentation\n\n\n\n\n\n\n\n\nDescription\n\n\nLink\n\n\n\n\n\n\n\n\n\n\nStable build documentation\n\n\nView JavaDoc", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "Welcome to the Karnaugh Map Simplifier software documentation site. The purpose of this website is to provide documentation for both developers and to users. This includes theoretical material on approaches to simplifying boolean algebra expressions both by hand and through computation.", 
            "title": "Welcome"
        }, 
        {
            "location": "/#history-and-objective", 
            "text": "The purpose of this software is to provide a collection of tools for engineers to simplify logical expressions and to visualize the results in a Karnaugh Map. There are two versions of the software. The first version, which was started in 2015 used Java Swing and supported up to four variable visual expression simplification. In the summer of 2017, a new version started development which is being made in JavaFX.", 
            "title": "History and Objective"
        }, 
        {
            "location": "/#future-goals", 
            "text": "The ultimate goal for this project is to become the standard industry tool in working with boolean algebra expressions. This includes simplification of large expressions, and working on converting expressions between different forms.", 
            "title": "Future Goals"
        }, 
        {
            "location": "/#download-links", 
            "text": "Description  Java Required  Link      Latest stable build  JRE 1.6  Download    Latest stable source  JDK 1.7  Download    Development source  JDK 1.8  GitHub", 
            "title": "Download Links"
        }, 
        {
            "location": "/#youtube-tutorials-demos", 
            "text": "Tutorial Topic  YouTube Link      General Software Demonstration  View video", 
            "title": "YouTube Tutorials &amp; Demos"
        }, 
        {
            "location": "/#javadoc-documentation", 
            "text": "Description  Link      Stable build documentation  View JavaDoc", 
            "title": "JavaDoc Documentation"
        }, 
        {
            "location": "/Boolean Algebra/", 
            "text": "Boolean Algebra and Operators\n\n\nBoolean algebra is a branch of algebra where the values of variables can only be \ntrue\n or \nfalse\n (often denoted by \n1\n and \n0\n respectfully). We use boolean algebra in circuits, general two-valued logic (such as in mathematics), and boolean operations.\n\n\nThere are multiple operations in boolean algebra, but the ones we will focus on are \nAND\n and \nOR\n. We denote \nAND\n through multiplication (ex: \nAB\n) and we denote \nOR\n through addition (ex: \nA+B\n). These operations follow the commutative property, meaning that the order in which we place the operands does not matter. We also have \nNOT\n, denoted by \n'\n operator, which flips the sign, for example \nA'\n.\n\n\nTruth Tables\n\n\nWhenever we have a boolean expression, we can express it as a function. Similar to \nF(x)\n we can express a two variable function: \nF(AB)\n, where we can say our domain is \n{A,B}\n. Since our input values can only be \ntrue\n or \nfalse\n, we can create a truth table that will show all possible cases with their inputs to the function and the result. Let's take a look at a simple two variable expression, and see how the logic gates \nAND\n and \nOR\n work. \n\n\nFor example, let's take the equation \nF(AB) = AB\n and generate the truth table: \n\n\n\n\n\n\n\n\nF(AB)\n\n\nA\n\n\nB\n\n\n\n\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nWe can also take the equation \nF(AB) = A+B\n and generate the truth table:\n\n\n\n\n\n\n\n\nF(A+B)\n\n\nA\n\n\nB\n\n\n\n\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nTruth tables are important because we can easily evaluate for the behavior of a function. For user input, it is easier to input truth tables because we can specify cases such as \nDon't Cares\n, which will be discussed later on. \n\n\nSum of Product Expressions (SOP)\n\n\nLet's consider a more complicated expression \nF(ABCD)= AB'C+BD+CD+D\n and generate its truth table:\n\n\n\n\n\n\n\n\nF(AB'C+BD+CD+D)\n\n\nA\n\n\nB\n\n\nC\n\n\nD\n\n\n\n\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n\n\n0\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nThis example was definately more involved than the previous expressions. An interesting observation is that we are doing a sum of product evaluation, that is, \nAB'C+BD+CD+D\n is a sum of products. The significance of a sum of product is that when we are doing \n+\n we are in fact invoking the \nOR\n operator. \n\n\nMoreover, the \nOR\n operator returns \ntrue\n so long as any one of its arguements returns \ntrue\n. Therefore, if \nany\n of the terms in the sum of product (SOP) expressions is \ntrue\n, then we know that the final expression is \ntrue\n for certain. \n\n\nExample Algebraic Simplification\n\n\nLet's simplify our expression from the previous truth table example. We can apply ordinary algebra tricks such as factoring. Remember that the \n+\n operator invokes the \nOR\n gate, and that \ntrue or x\n always returns \ntrue\n regardless of \nx\n (as shown in our first truth table).\n\n\nAB'C+BD+CD+D // Initial expression\nAB'C+BD+D(C+1) // Factor out a D\nAB'C+BD+D // Since (C+1) is always true, as C OR true is always true\nAB'C+D(B+1) // Factor out a D again\nAB'C+D // Since (B+1) is always true, as B OR true is always true\n=AB'C+D // Final expression\n\n\n\n\nAs an exercise to the reader, complete the truth table to show that they are logically equivalent. \n\n\nUndefined Input \n Don't Cares\n\n\nThe definition of a \"Don't care\" is a combination of input values that is not known, and could be either \n0\n or \n1\n. For the purposes of variable simplification, we would choose the greedy approach of picking between {\n0\n, \n1\n} such that the simplified expression has less terms.\n\n\nLet's consider the following truth-table:\n\n\n\n\n\n\n\n\nF(AB)\n\n\nA\n\n\nB\n\n\n\n\n\n\n\n\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n\n\n1\n\n\n0\n\n\n1\n\n\n\n\n\n\n?\n\n\n1\n\n\n0\n\n\n\n\n\n\n1\n\n\n1\n\n\n1\n\n\n\n\n\n\n\n\nWe observe that we have a \nDon't care\n. Let's observe the differences in cases for \nF(1,0)\n:\n\n\nCase #1: F(1, 0) = 0\n=\n F(AB) = A'B' + A'B + AB\n\nCase #2: F(1, 0) = 1\n=\n F(AB) = A'B' + A'B + AB' + AB\n\nSimplifying the cases...\nF(AB) = A'B' + A'B + AB\n      = A'(B' + B) + AB\n      = A' + AB\nF(AB) = A'B' + A'B + AB' + AB\n      = A'(B' + B) + A (B' + B)\n      = A' + A\n      = 1\n\n\n\n\nWe can clearly see, if we set \nF(1, 0) = 1\n, we get a true value for any input. Therefore, for the purposes of variable simplification, we can simply let \nF(1, 0) = 1\n thus implying \nF(AB) = 1\n.\n\n\nAdditional Logic Gates\n\n\nSo far we covered the \nNOT\n unary operator, in addition to the binary operators \nOR\n and \nAND\n.", 
            "title": "Boolean Algebra"
        }, 
        {
            "location": "/Boolean Algebra/#boolean-algebra-and-operators", 
            "text": "Boolean algebra is a branch of algebra where the values of variables can only be  true  or  false  (often denoted by  1  and  0  respectfully). We use boolean algebra in circuits, general two-valued logic (such as in mathematics), and boolean operations.  There are multiple operations in boolean algebra, but the ones we will focus on are  AND  and  OR . We denote  AND  through multiplication (ex:  AB ) and we denote  OR  through addition (ex:  A+B ). These operations follow the commutative property, meaning that the order in which we place the operands does not matter. We also have  NOT , denoted by  '  operator, which flips the sign, for example  A' .", 
            "title": "Boolean Algebra and Operators"
        }, 
        {
            "location": "/Boolean Algebra/#truth-tables", 
            "text": "Whenever we have a boolean expression, we can express it as a function. Similar to  F(x)  we can express a two variable function:  F(AB) , where we can say our domain is  {A,B} . Since our input values can only be  true  or  false , we can create a truth table that will show all possible cases with their inputs to the function and the result. Let's take a look at a simple two variable expression, and see how the logic gates  AND  and  OR  work.   For example, let's take the equation  F(AB) = AB  and generate the truth table:      F(AB)  A  B      0  0  0    0  0  1    0  1  0    1  1  1     We can also take the equation  F(AB) = A+B  and generate the truth table:     F(A+B)  A  B      0  0  0    1  0  1    1  1  0    1  1  1     Truth tables are important because we can easily evaluate for the behavior of a function. For user input, it is easier to input truth tables because we can specify cases such as  Don't Cares , which will be discussed later on.", 
            "title": "Truth Tables"
        }, 
        {
            "location": "/Boolean Algebra/#sum-of-product-expressions-sop", 
            "text": "Let's consider a more complicated expression  F(ABCD)= AB'C+BD+CD+D  and generate its truth table:     F(AB'C+BD+CD+D)  A  B  C  D      0  0  0  0  0    1  0  0  0  1    0  0  0  1  0    1  0  0  1  1    0  0  1  0  0    1  0  1  0  1    0  0  1  1  0    1  0  1  1  1    0  1  0  0  0    1  1  0  0  1    0  1  0  1  0    1  1  0  1  1    1  1  1  0  0    1  1  1  0  1    0  1  1  1  0    1  1  1  1  1     This example was definately more involved than the previous expressions. An interesting observation is that we are doing a sum of product evaluation, that is,  AB'C+BD+CD+D  is a sum of products. The significance of a sum of product is that when we are doing  +  we are in fact invoking the  OR  operator.   Moreover, the  OR  operator returns  true  so long as any one of its arguements returns  true . Therefore, if  any  of the terms in the sum of product (SOP) expressions is  true , then we know that the final expression is  true  for certain.", 
            "title": "Sum of Product Expressions (SOP)"
        }, 
        {
            "location": "/Boolean Algebra/#example-algebraic-simplification", 
            "text": "Let's simplify our expression from the previous truth table example. We can apply ordinary algebra tricks such as factoring. Remember that the  +  operator invokes the  OR  gate, and that  true or x  always returns  true  regardless of  x  (as shown in our first truth table).  AB'C+BD+CD+D // Initial expression\nAB'C+BD+D(C+1) // Factor out a D\nAB'C+BD+D // Since (C+1) is always true, as C OR true is always true\nAB'C+D(B+1) // Factor out a D again\nAB'C+D // Since (B+1) is always true, as B OR true is always true\n=AB'C+D // Final expression  As an exercise to the reader, complete the truth table to show that they are logically equivalent.", 
            "title": "Example Algebraic Simplification"
        }, 
        {
            "location": "/Boolean Algebra/#undefined-input-dont-cares", 
            "text": "The definition of a \"Don't care\" is a combination of input values that is not known, and could be either  0  or  1 . For the purposes of variable simplification, we would choose the greedy approach of picking between { 0 ,  1 } such that the simplified expression has less terms.  Let's consider the following truth-table:     F(AB)  A  B      1  0  0    1  0  1    ?  1  0    1  1  1     We observe that we have a  Don't care . Let's observe the differences in cases for  F(1,0) :  Case #1: F(1, 0) = 0\n=  F(AB) = A'B' + A'B + AB\n\nCase #2: F(1, 0) = 1\n=  F(AB) = A'B' + A'B + AB' + AB\n\nSimplifying the cases...\nF(AB) = A'B' + A'B + AB\n      = A'(B' + B) + AB\n      = A' + AB\nF(AB) = A'B' + A'B + AB' + AB\n      = A'(B' + B) + A (B' + B)\n      = A' + A\n      = 1  We can clearly see, if we set  F(1, 0) = 1 , we get a true value for any input. Therefore, for the purposes of variable simplification, we can simply let  F(1, 0) = 1  thus implying  F(AB) = 1 .", 
            "title": "Undefined Input &amp; Don't Cares"
        }, 
        {
            "location": "/Boolean Algebra/#additional-logic-gates", 
            "text": "So far we covered the  NOT  unary operator, in addition to the binary operators  OR  and  AND .", 
            "title": "Additional Logic Gates"
        }, 
        {
            "location": "/Karnaugh Maps/", 
            "text": "Introduction\n\n\nKarnaugh Maps are a way to visually display a boolean expression onto a 2D grid. We take the variables and bind them to an axis, and then enumerate through the possible combinations of input values that could occur for all those variables bounded to an axis (either horizontally or vertically).\n\n\nFor example, we can display the following 2 variable Karnaugh Map:\n\n\n\n\nWe have bounded to the vertical axis, the variable \nA\n, and we enumerate through the possible values for \nA\n (being \n{0, 1}\n). Similarily, we perform a similar operation for the \nB\n variable. Since we are using a 2 variable expression, we can bound one variable to each axis and the visualization works fine in a \n2x2\n matrix.\n\n\nLet's instead look at a more involved example with 4 variables:\n\n\n\n\nWe have now bounded the \nA\n and \nB\n variables to the vertical axis, while we bounded the \nC\n and \nD\n variables to the horizontal axis. We now enumarate through different combinations of the bounded variables for each axis in \nreflected binary code order\n (more on this in the following section). Lastly, we indicate on the matrix each true value by augmenting a \n1\n value.\n\n\nEnumeration and Gray Codes\n\n\nWhen enumerating through the variable input combinations for the binded axis, we take advantage of \nreflected binary code order\n, otherwise known as gray codes. If we observe carefully, we can notice that from one combination to another, we only vary by one bit. That is:\n\n\n... 00 01 11 10 00 01 11 10 00 ...\n    ^   ^ ^   ^ ^   ^ ^   ^ ^\n\n\n\n\nThus, we get this wrapping that allows us to switch by only one bit. This provides us the core for how Karnaugh Maps work.\n\n\nSimple Groupings\n\n\nThe main idea for how Karnaugh Maps can be used to simplify expressions is to group pairs of \n1\n values that are adjacent, and exploit the fact that each one has only a bit difference from another. \n\n\n\n\nFor the purpose of this example, let \nF(ABCD) = CELL\n. We start with the expression \nF(0000) = 1\n and \nF(0001) = 1\n. However, notice that \nregardless\n of the value of the last bit, we still get \n1\n. Hence, let's take a look at the SOP expressions:\n\n\nF(ABCD) = A'B'C'D' + A'B'C'D\nF(0000) = 1\nF(0001) = 1\n\nSince the last bit is the same, we can ignore the D value, thus:\nF(ABCD) = A'B'C'\n\nWe can confirm by simplifying algebraically:\nF(ABCD) = A'B'C'D' + A'B'C'D\n        = A'B'C'(D' + D)\n        = A'B'C'\nTherefore, the simplification is true.\n\n\n\n\nWe can then extend this rule to work for rectangles and more!\n\n\nTwo Dimension Groupings\n\n\nExtending the idea of isolating changing bits that retain a consistent value, we can then generalize this to work in a higher dimension. Consider the following example:\n\n\n\n\nLetting \nF(ABCD) = CELL\n:\n\n\nF(0000) = 1\nF(0001) = 1\nF(0100) = 1\nF(0101) = 1\n\n\n\n\nObserve that the bits do not change by one for all pairs of numbers, for example \n{0000, 0101}\n differ by two bits. However, we can take advantage of the fact that for any bit change horizontally or vertically, it's irrelvant what that bit is. More concretely, take a look at the following example.\n\n\n0000 0001\n0100 0101\n\n=\n A'B'C'D' + A'B'C'D + A'BC'D' + A'BC'D\nRegardless of the B variable, we still get true for all products in the SOP expression.\nThis is bounded vertically:\n=\n A'C'D' + A'C'D + A'C'D' + A'C'D\nRegardless of the D variable, we still get true for all products in the SOP expression.\nThis is bounded horizontally:\n=\n A'C' + A'C' + A'C' + A'C'\n=\n A'C' (1 + 1 + 1 + 1)\n=\n A'C' (1)\n=\n A'C'\n\n\n\n\nSince the differences in bits needs to generalize throughout a binding of an axis, you can only have a binding of size \n2^n\n for a given axis. For example, \n1x1, 1x2, 1x4, 2x2, 2x4, 4x4\n. \n\n\nDisjoint Groupings\n\n\nConsider the following example:\n\n\n\nThe algorithm follows precisely as it did before, except that now the two groups are joined in the SOP expression. Letting \nF(ABCD) = CELL\n:\n\n\nF(0000) = 1\nF(0001) = 1\nF(1111) = 1\nF(1011) = 1\n\n\n\n\nThis yields the following:\n\n\nA'B'C'D' + A'B'C'D + ABCD + AB'CD\nBreaking down the expression:\n(A'B'C'D' + A'B'C'D) + (ABCD + AB'CD)\n=\n (A'B'C'(D + D')) + (ACD(B + B'))\n=\n (A'B'C') + (ACD)\n=\n A'B'C' + ACD\n\n\n\n\nClearly this is the exact same process as before, but iterated throughout all the disjoint sets.\n\n\nOverlapping Groupings\n\n\nOverlapping groupings become more complex, because there exist ambigious cases and sometimes what may appear to be a locally optimal solutuion is not a globally optimal solution.\n\n\nThe general technique for evaluating for overlapping groups follows a greedy algorithm. Define an unvisited cell as a cell that has a value of \n1\n however it is currently not matched with a grouping yet. \n\n\nIterate through all the cells, and once you find a cell with \n1\n, if it is unvisited then find the largest possible square or rectangle such that each side length is a power of 2, where all the cells are \n1\n in its enclosed area. If there is a tie for size (ie, \n1x4\n vs \n2x2\n), assign the one that is a square (this is by convention). \n\n\nRepeat this process for all remaining unvisited cells. Note: You can overlap the groupings with already visited nodes, but you never instantiate a new grouping unless the current node is unvisited.\n\n\n\n\nIn this example, at \nF(0000)\n, we can create a grouping of size 2 (because 2 is the largest possible grouping, 3 is not a power of 2). We then iterate through to \nF(0001)\n, however \nF(0001)\n was already resolved to a grouping. For the latest active cell, \nF(0011)\n is not resolved to a grouping thus it's unvisited. The largest possible grouping is also of size 2, thus we create another group.\n\n\nTo resolve the groupings into an SOP expression, we iterate through the groups and identify changing bits:\n\n\nGroup #1 =\n F(ABCD) = [0000, 0001]\nGroup #2 =\n F(ABCD) = [0001, 0011]\n\nFor Group #1:\n0000 0001\n   ^    ^\nF(ABCD) = A'B'C'D' + A'B'C'D\n=\n A'B'C'(D + D')\n=\n A'B'C'\n\nFor Group #2:\n0001 0011\n  ^    ^\nF(ABCD) = A'B'C'D + A'B'CD\n=\n A'B'D(C' + C)\n=\n A'B'D\n\nNow we add the two results:\nF(ABCD) = A'B'C' + A'B'D\n=\n F(ABCD) = A'B'D + A'B'C' (by commutative property)\n\n\n\n\nMinimizing Group Count\n\n\nThe following example will ilustrate how the greedy approach may occasionally produce too many groups. Consider the following example:\n\n\n\n\nThis grouping state is optimal. However, consider adding a \n1\n to \nF(1111)\n.\n\n\n\n\nFollowing the previous algorithm, iterating top-bottom and left-right, when getting to \nF(0110)\n, the algorithm can choose to make the largest grouping. However, there are two possible groupings:\n\n\nCandidate #1:\nF(ABCD) = [0011, 0010, 0111, 0110]\n\nCandidate #2:\nF(ABCD) = [0111, 0110, 1111, 1110]\n\n\n\n\nBoth groupings have the same size, and are the same dimension. However, upon reaching \nF(1110)\n, another grouping needs to be instantiated, in which case if the first candidate grouping was created then we made a group that was not neccessary increasing the size of our SOP expression. \n\n\nThis illustrates the idea that this is a greedy algorithm, and does not always return the most simplified SOP expression. In later sections, algorithms illustrating a globally optimal algorithm will be discussed.", 
            "title": "Karnaugh Maps"
        }, 
        {
            "location": "/Karnaugh Maps/#introduction", 
            "text": "Karnaugh Maps are a way to visually display a boolean expression onto a 2D grid. We take the variables and bind them to an axis, and then enumerate through the possible combinations of input values that could occur for all those variables bounded to an axis (either horizontally or vertically).  For example, we can display the following 2 variable Karnaugh Map:   We have bounded to the vertical axis, the variable  A , and we enumerate through the possible values for  A  (being  {0, 1} ). Similarily, we perform a similar operation for the  B  variable. Since we are using a 2 variable expression, we can bound one variable to each axis and the visualization works fine in a  2x2  matrix.  Let's instead look at a more involved example with 4 variables:   We have now bounded the  A  and  B  variables to the vertical axis, while we bounded the  C  and  D  variables to the horizontal axis. We now enumarate through different combinations of the bounded variables for each axis in  reflected binary code order  (more on this in the following section). Lastly, we indicate on the matrix each true value by augmenting a  1  value.", 
            "title": "Introduction"
        }, 
        {
            "location": "/Karnaugh Maps/#enumeration-and-gray-codes", 
            "text": "When enumerating through the variable input combinations for the binded axis, we take advantage of  reflected binary code order , otherwise known as gray codes. If we observe carefully, we can notice that from one combination to another, we only vary by one bit. That is:  ... 00 01 11 10 00 01 11 10 00 ...\n    ^   ^ ^   ^ ^   ^ ^   ^ ^  Thus, we get this wrapping that allows us to switch by only one bit. This provides us the core for how Karnaugh Maps work.", 
            "title": "Enumeration and Gray Codes"
        }, 
        {
            "location": "/Karnaugh Maps/#simple-groupings", 
            "text": "The main idea for how Karnaugh Maps can be used to simplify expressions is to group pairs of  1  values that are adjacent, and exploit the fact that each one has only a bit difference from another.    For the purpose of this example, let  F(ABCD) = CELL . We start with the expression  F(0000) = 1  and  F(0001) = 1 . However, notice that  regardless  of the value of the last bit, we still get  1 . Hence, let's take a look at the SOP expressions:  F(ABCD) = A'B'C'D' + A'B'C'D\nF(0000) = 1\nF(0001) = 1\n\nSince the last bit is the same, we can ignore the D value, thus:\nF(ABCD) = A'B'C'\n\nWe can confirm by simplifying algebraically:\nF(ABCD) = A'B'C'D' + A'B'C'D\n        = A'B'C'(D' + D)\n        = A'B'C'\nTherefore, the simplification is true.  We can then extend this rule to work for rectangles and more!", 
            "title": "Simple Groupings"
        }, 
        {
            "location": "/Karnaugh Maps/#two-dimension-groupings", 
            "text": "Extending the idea of isolating changing bits that retain a consistent value, we can then generalize this to work in a higher dimension. Consider the following example:   Letting  F(ABCD) = CELL :  F(0000) = 1\nF(0001) = 1\nF(0100) = 1\nF(0101) = 1  Observe that the bits do not change by one for all pairs of numbers, for example  {0000, 0101}  differ by two bits. However, we can take advantage of the fact that for any bit change horizontally or vertically, it's irrelvant what that bit is. More concretely, take a look at the following example.  0000 0001\n0100 0101\n\n=  A'B'C'D' + A'B'C'D + A'BC'D' + A'BC'D\nRegardless of the B variable, we still get true for all products in the SOP expression.\nThis is bounded vertically:\n=  A'C'D' + A'C'D + A'C'D' + A'C'D\nRegardless of the D variable, we still get true for all products in the SOP expression.\nThis is bounded horizontally:\n=  A'C' + A'C' + A'C' + A'C'\n=  A'C' (1 + 1 + 1 + 1)\n=  A'C' (1)\n=  A'C'  Since the differences in bits needs to generalize throughout a binding of an axis, you can only have a binding of size  2^n  for a given axis. For example,  1x1, 1x2, 1x4, 2x2, 2x4, 4x4 .", 
            "title": "Two Dimension Groupings"
        }, 
        {
            "location": "/Karnaugh Maps/#disjoint-groupings", 
            "text": "Consider the following example:  The algorithm follows precisely as it did before, except that now the two groups are joined in the SOP expression. Letting  F(ABCD) = CELL :  F(0000) = 1\nF(0001) = 1\nF(1111) = 1\nF(1011) = 1  This yields the following:  A'B'C'D' + A'B'C'D + ABCD + AB'CD\nBreaking down the expression:\n(A'B'C'D' + A'B'C'D) + (ABCD + AB'CD)\n=  (A'B'C'(D + D')) + (ACD(B + B'))\n=  (A'B'C') + (ACD)\n=  A'B'C' + ACD  Clearly this is the exact same process as before, but iterated throughout all the disjoint sets.", 
            "title": "Disjoint Groupings"
        }, 
        {
            "location": "/Karnaugh Maps/#overlapping-groupings", 
            "text": "Overlapping groupings become more complex, because there exist ambigious cases and sometimes what may appear to be a locally optimal solutuion is not a globally optimal solution.  The general technique for evaluating for overlapping groups follows a greedy algorithm. Define an unvisited cell as a cell that has a value of  1  however it is currently not matched with a grouping yet.   Iterate through all the cells, and once you find a cell with  1 , if it is unvisited then find the largest possible square or rectangle such that each side length is a power of 2, where all the cells are  1  in its enclosed area. If there is a tie for size (ie,  1x4  vs  2x2 ), assign the one that is a square (this is by convention).   Repeat this process for all remaining unvisited cells. Note: You can overlap the groupings with already visited nodes, but you never instantiate a new grouping unless the current node is unvisited.   In this example, at  F(0000) , we can create a grouping of size 2 (because 2 is the largest possible grouping, 3 is not a power of 2). We then iterate through to  F(0001) , however  F(0001)  was already resolved to a grouping. For the latest active cell,  F(0011)  is not resolved to a grouping thus it's unvisited. The largest possible grouping is also of size 2, thus we create another group.  To resolve the groupings into an SOP expression, we iterate through the groups and identify changing bits:  Group #1 =  F(ABCD) = [0000, 0001]\nGroup #2 =  F(ABCD) = [0001, 0011]\n\nFor Group #1:\n0000 0001\n   ^    ^\nF(ABCD) = A'B'C'D' + A'B'C'D\n=  A'B'C'(D + D')\n=  A'B'C'\n\nFor Group #2:\n0001 0011\n  ^    ^\nF(ABCD) = A'B'C'D + A'B'CD\n=  A'B'D(C' + C)\n=  A'B'D\n\nNow we add the two results:\nF(ABCD) = A'B'C' + A'B'D\n=  F(ABCD) = A'B'D + A'B'C' (by commutative property)", 
            "title": "Overlapping Groupings"
        }, 
        {
            "location": "/Karnaugh Maps/#minimizing-group-count", 
            "text": "The following example will ilustrate how the greedy approach may occasionally produce too many groups. Consider the following example:   This grouping state is optimal. However, consider adding a  1  to  F(1111) .   Following the previous algorithm, iterating top-bottom and left-right, when getting to  F(0110) , the algorithm can choose to make the largest grouping. However, there are two possible groupings:  Candidate #1:\nF(ABCD) = [0011, 0010, 0111, 0110]\n\nCandidate #2:\nF(ABCD) = [0111, 0110, 1111, 1110]  Both groupings have the same size, and are the same dimension. However, upon reaching  F(1110) , another grouping needs to be instantiated, in which case if the first candidate grouping was created then we made a group that was not neccessary increasing the size of our SOP expression.   This illustrates the idea that this is a greedy algorithm, and does not always return the most simplified SOP expression. In later sections, algorithms illustrating a globally optimal algorithm will be discussed.", 
            "title": "Minimizing Group Count"
        }, 
        {
            "location": "/Simplification Examples/", 
            "text": "Simplification Examples\n\n\nMore examples coming soon.", 
            "title": "Simplification Examples"
        }, 
        {
            "location": "/Simplification Examples/#simplification-examples", 
            "text": "More examples coming soon.", 
            "title": "Simplification Examples"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/", 
            "text": "Algorithm Overview\n\n\nThe process of simplifying variables essentially involves greedily obtaining larger rectangles to encompass as many \n1\n unit within a grid. To ensure that we cover all the possible pairings, we need to devise an algorithm to generate a sample space containing all possible outcomes, and then we can pick out the pairings that we need from there.\n\n\nThe process begins by plotting the points \n0\n and \n1\n in the Karnaugh Map. We can assume that the up left corner in \n(x,y)\n coordinate is \n(a,b)\n and bottom right corner is \n(c,d)\n. An interesting property is that:\n\n\n\n\nConsider the grouping \n(a,b)\n to \n(c,d)\n. The group is valid if and only if sum of all \n1\ns in the rectangle is equal to \n(c-a)*(d-b)\n.\n\n\n\n\nThe significance of this property is that we can quickly identify if a grouping is valid or not in \nO(1)\n time. Thus, we can exhaust all possible scenarios and we saved a linear (or quadratic) factor.\n\n\nFundamental Theorem of Calculus\n\n\nThe first part of the fundamental theorem of calculus claims that we can take the anti-derivative of a function \n\u0192\n, and find the area under the curve from point \nA\n to \nB\n by doing \nF(A)-F(B)\n. Extending this definition to arrays, we can generate an anti-derivative for an array and find the summation from point \nA\n to \nB\n in \nO(1)\n time.\n\n\nConsider:\n\n\narr = [1, 0, 1, 1, 0, 1]\narr* = [0, 1, 1, 2, 3, 3, 4]\n\n\n\n\nWe can then find the summation from item \narr[i]\n for \ni = [a, b]\n by doing:\n\n\nsum = arr*[b+1] - arr*[a];\n\n\n\n\nThis alow us to sum up values very quickly, and extending this to two dimensions we can rule out impossible cases.\n\n\nRemark: Notice the length of the array is one index larger.\n\n\nPrinciple of Inclusion-Exclusion\n\n\nDue to the nature of Karnaugh Maps, we would want to be able to do summation in two dimensions. This can be achieved using the principle of inclusion-exclusion. \n\n\nApplying the exact same algorithm as before, except now the size of the array is one larger on the horizontal axis and one larger on the vertical access as well. \n\n\nThe final formula for retrieving the area is:\n\n\nsum = arr*[d+1][c+1] - arr*[d+1][c] - arr*[d][c+1] + arr*[b][a];\n\n\n\n\nPriority Groupings + Heuristic\n\n\nGenerating valid groupings becomes trivial upon being able to check the validity using 2D prefix sum arrays. Iterate through all the \n(x,y)\n pairs, then try all possible starting and ending coordinates. This operation will take \nO((NM)^2)\n, where \nN\n = # of rows, \nM\n = # of columns.\n\n\nWhen deciding the final groupings, insert all of the candidate groupings as an encapsulated object into a priority queue. The key in the priority queue used will be the size of the grouping. If two groupings are equal in size, the one with the more \"square\" shape will get a higher priority. The method for determining the \"square-ness\" of a candidate is the absolute difference in its width and height.\n\n\nApproach Drawbacks\n\n\nThe first drawback for this approach is speed. Despite being able to eliminate all solutions that don't satisfy the rectangular property of a grouping in \nO(1)\n time, we still exhaust all possibilities. Moreover, we also store the entire solution space in a priority queue which is very memory intensive.\n\n\nMemory Optimization: Cell-Wise Priority Queues\n\n\nAn important observation to make is that we are only interested in the largest possible grouping from each particular cell. This means that during each iteration of the algorithm, we should have at most \nNxM\n items in our priority queue, because the rest aren't useful! We can extend this idea by simply have a cell-wise 2D array that represents the grid used by cell, that is, a grouping that warrants its existence because it is the largest grouping to satisfy a given cell.\n\n\nThis however adds a factor of time complexity, how do we know that a rectangular grouping is a maximum for any of the cells within the area it surrounds? One approach is to manually check each cell, but it isn't good because it takes \nNxM\n amount of time. \n\n\nWe could also, when generating the groups, start at the best cases, and keep iterating until you get to the worst case, but this presents another problem because if it's the only cell, then we do \nNxM\n iterations. To make things worse, we have to repeat this for all rows/cols, thus we will end up with \nN^2*M^2\n. The following would be a worse case, as we don't quit any options we're exhausting to until we end up with just 1 size grouping:", 
            "title": "Efficient Exhaustion Algorithm"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/#algorithm-overview", 
            "text": "The process of simplifying variables essentially involves greedily obtaining larger rectangles to encompass as many  1  unit within a grid. To ensure that we cover all the possible pairings, we need to devise an algorithm to generate a sample space containing all possible outcomes, and then we can pick out the pairings that we need from there.  The process begins by plotting the points  0  and  1  in the Karnaugh Map. We can assume that the up left corner in  (x,y)  coordinate is  (a,b)  and bottom right corner is  (c,d) . An interesting property is that:   Consider the grouping  (a,b)  to  (c,d) . The group is valid if and only if sum of all  1 s in the rectangle is equal to  (c-a)*(d-b) .   The significance of this property is that we can quickly identify if a grouping is valid or not in  O(1)  time. Thus, we can exhaust all possible scenarios and we saved a linear (or quadratic) factor.", 
            "title": "Algorithm Overview"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/#fundamental-theorem-of-calculus", 
            "text": "The first part of the fundamental theorem of calculus claims that we can take the anti-derivative of a function  \u0192 , and find the area under the curve from point  A  to  B  by doing  F(A)-F(B) . Extending this definition to arrays, we can generate an anti-derivative for an array and find the summation from point  A  to  B  in  O(1)  time.  Consider:  arr = [1, 0, 1, 1, 0, 1]\narr* = [0, 1, 1, 2, 3, 3, 4]  We can then find the summation from item  arr[i]  for  i = [a, b]  by doing:  sum = arr*[b+1] - arr*[a];  This alow us to sum up values very quickly, and extending this to two dimensions we can rule out impossible cases.  Remark: Notice the length of the array is one index larger.", 
            "title": "Fundamental Theorem of Calculus"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/#principle-of-inclusion-exclusion", 
            "text": "Due to the nature of Karnaugh Maps, we would want to be able to do summation in two dimensions. This can be achieved using the principle of inclusion-exclusion.   Applying the exact same algorithm as before, except now the size of the array is one larger on the horizontal axis and one larger on the vertical access as well.   The final formula for retrieving the area is:  sum = arr*[d+1][c+1] - arr*[d+1][c] - arr*[d][c+1] + arr*[b][a];", 
            "title": "Principle of Inclusion-Exclusion"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/#priority-groupings-heuristic", 
            "text": "Generating valid groupings becomes trivial upon being able to check the validity using 2D prefix sum arrays. Iterate through all the  (x,y)  pairs, then try all possible starting and ending coordinates. This operation will take  O((NM)^2) , where  N  = # of rows,  M  = # of columns.  When deciding the final groupings, insert all of the candidate groupings as an encapsulated object into a priority queue. The key in the priority queue used will be the size of the grouping. If two groupings are equal in size, the one with the more \"square\" shape will get a higher priority. The method for determining the \"square-ness\" of a candidate is the absolute difference in its width and height.", 
            "title": "Priority Groupings + Heuristic"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/#approach-drawbacks", 
            "text": "The first drawback for this approach is speed. Despite being able to eliminate all solutions that don't satisfy the rectangular property of a grouping in  O(1)  time, we still exhaust all possibilities. Moreover, we also store the entire solution space in a priority queue which is very memory intensive.", 
            "title": "Approach Drawbacks"
        }, 
        {
            "location": "/Efficient Exhaustion Algorithm/#memory-optimization-cell-wise-priority-queues", 
            "text": "An important observation to make is that we are only interested in the largest possible grouping from each particular cell. This means that during each iteration of the algorithm, we should have at most  NxM  items in our priority queue, because the rest aren't useful! We can extend this idea by simply have a cell-wise 2D array that represents the grid used by cell, that is, a grouping that warrants its existence because it is the largest grouping to satisfy a given cell.  This however adds a factor of time complexity, how do we know that a rectangular grouping is a maximum for any of the cells within the area it surrounds? One approach is to manually check each cell, but it isn't good because it takes  NxM  amount of time.   We could also, when generating the groups, start at the best cases, and keep iterating until you get to the worst case, but this presents another problem because if it's the only cell, then we do  NxM  iterations. To make things worse, we have to repeat this for all rows/cols, thus we will end up with  N^2*M^2 . The following would be a worse case, as we don't quit any options we're exhausting to until we end up with just 1 size grouping:", 
            "title": "Memory Optimization: Cell-Wise Priority Queues"
        }, 
        {
            "location": "/Dynamic Programming/", 
            "text": "Dynamic Programming\n\n\nDynamic programming is a technique used in solving problems bottom-up (or top-down), and use solutions from overlapping subproblems to build up our larger solutions. In this example, if we figure out that we can make a small 2x2 grouping, can we use that to form larger groupings?\n\n\nKadane's Algorithm in 1D\n\n\nKadane's algorithm finds the largest subarray in an array containing numbers in O(N) time with O(1) space. For example:\n\n\n[3, 6, -1, 3, -6, -7, 3, 15, -432, 2]\nLargest subarray =\n [3, 15] =\n 3+15 =\n 18\n\n\n\n\nThis can be computed efficient by taking into consideration the following facts which are at the core of Kadane's algorithm:\n\n\n\n\nStart iterating through the array\n\n\nKeep a cumulative sum\n\n\nIf, so far, our sum is positive, we should keep our elements\n\n\nIf we get a negative number, then we should start over\n\n\n\n\nThis is efficient, because at each iteration of Kadane's algorithm, we don't have to look back, and we use constant memory. A naive solution would have a complexity of \nO(2^N)\n\n\nRelating Kadane's Algorithm to Karnaugh Maps\n\n\nIf we can extend Kadane's Algorithm to 2D, and instead find the largest submatrices such that all rows/columns are filled, we would solve our problem. How can this be solved? First, consider the Karnaugh Map is a grid of \n0's\n and \n1's\n. From before, we know that it is a filled matrix if and only if the sum is equal to width * height.\n\n\nHence, we need to extend Kadane's algorithm to 2D, taking note of the perfectly filled matrices.\n\n\nExtending Algorithm to 2D\n\n\nThe following describes the algorithm, which takes \nO(N^2*M)\n time:\n\n\n\n\nStore the cumulative sums of all columns in the matrix. That is, the top-most row should contain just that item, and the row below should contain that item plus the item at its row/columns\n\n\nStart iterating through from the top row to the bottom row, called \nx1\n\n\nStart another loop, which iterates from x1 to the bottom as well, called \nx2\n\n\nSubtract, for all columns, the difference in height between \nx2\n and \nx1\n. This is the sum using the prefix sums array\n\n\nFor the produced difference array, run 1D Kadane's algorithm\n\n\nWhen runnning Kadane's algorithm, do not include results who have different consecutive values in the 1D array. These potential matrices have holes with \n0\n. Also ignore cases of illegal sizes.\n\n\n\n\nThe end result contains the largest matrix possible. To store the results, you can store them in a data-structure like a heap and sort based on \nMath.abs(x1-x2) * Math.abs(y1-y2)\n\n\nFor example:\n\n\n[0, 1, 0, 0      =\n         [0, 1, 0, 0\n 1, 1, 1, 1      =\n          1, 2, 1, 1 \n 0, 1, 1, 1      =\n          1, 3, 2, 1\n 0, 0, 1, 0]     =\n          1, 3, 3, 1]\n\n\n\n\n\nFollowing the algorithm previously illustrated, we first store the cumulative sums going down the columns. We then iterate through the indices with our \nx1\n and \nx2\n pointers.\n\n\nSo we first get \n[0, 1, 0, 0]\n as our array, with \nx1 = 0\n and \nx2 = 1\n, which we consider \n1\n to be the largest. We know it is \n1x1\n in size because (\nx2-x1=1\n) and we cover a range of \n1\n.\n\n\nWe then extend this further, increasing \nx2\n until we get to \n4\n, then increment \nx1\n by one and repeat, resulting in \nO(N^2)\n time before invoking 1D Kadane's algorithm, which takes \nO(N)\n time.\n\n\nWhen we get \nx1=1\n and \nx2=3\n, we have \n[0, 2, 2, 1]\n, which gives us \n[2, 2]\n with \nx2-x1=2\n, hence we have our maximum here being a grouping of 4.\n\n\nStoring Results\n\n\nWe can have several algorithms for storing the results. The easiest way is to store another 2D grid with the maximum pairing so far that encompasses that specific grid cell. If your new candidate is larger and it satisfies, you can set that grid. Else, you can discard it unless another cell requires it to be occupied. This will result in \nO(N*M)\n memory, which is optimal.", 
            "title": "Dynamic Programming"
        }, 
        {
            "location": "/Dynamic Programming/#dynamic-programming", 
            "text": "Dynamic programming is a technique used in solving problems bottom-up (or top-down), and use solutions from overlapping subproblems to build up our larger solutions. In this example, if we figure out that we can make a small 2x2 grouping, can we use that to form larger groupings?", 
            "title": "Dynamic Programming"
        }, 
        {
            "location": "/Dynamic Programming/#kadanes-algorithm-in-1d", 
            "text": "Kadane's algorithm finds the largest subarray in an array containing numbers in O(N) time with O(1) space. For example:  [3, 6, -1, 3, -6, -7, 3, 15, -432, 2]\nLargest subarray =  [3, 15] =  3+15 =  18  This can be computed efficient by taking into consideration the following facts which are at the core of Kadane's algorithm:   Start iterating through the array  Keep a cumulative sum  If, so far, our sum is positive, we should keep our elements  If we get a negative number, then we should start over   This is efficient, because at each iteration of Kadane's algorithm, we don't have to look back, and we use constant memory. A naive solution would have a complexity of  O(2^N)", 
            "title": "Kadane's Algorithm in 1D"
        }, 
        {
            "location": "/Dynamic Programming/#relating-kadanes-algorithm-to-karnaugh-maps", 
            "text": "If we can extend Kadane's Algorithm to 2D, and instead find the largest submatrices such that all rows/columns are filled, we would solve our problem. How can this be solved? First, consider the Karnaugh Map is a grid of  0's  and  1's . From before, we know that it is a filled matrix if and only if the sum is equal to width * height.  Hence, we need to extend Kadane's algorithm to 2D, taking note of the perfectly filled matrices.", 
            "title": "Relating Kadane's Algorithm to Karnaugh Maps"
        }, 
        {
            "location": "/Dynamic Programming/#extending-algorithm-to-2d", 
            "text": "The following describes the algorithm, which takes  O(N^2*M)  time:   Store the cumulative sums of all columns in the matrix. That is, the top-most row should contain just that item, and the row below should contain that item plus the item at its row/columns  Start iterating through from the top row to the bottom row, called  x1  Start another loop, which iterates from x1 to the bottom as well, called  x2  Subtract, for all columns, the difference in height between  x2  and  x1 . This is the sum using the prefix sums array  For the produced difference array, run 1D Kadane's algorithm  When runnning Kadane's algorithm, do not include results who have different consecutive values in the 1D array. These potential matrices have holes with  0 . Also ignore cases of illegal sizes.   The end result contains the largest matrix possible. To store the results, you can store them in a data-structure like a heap and sort based on  Math.abs(x1-x2) * Math.abs(y1-y2)  For example:  [0, 1, 0, 0      =          [0, 1, 0, 0\n 1, 1, 1, 1      =           1, 2, 1, 1 \n 0, 1, 1, 1      =           1, 3, 2, 1\n 0, 0, 1, 0]     =           1, 3, 3, 1]  Following the algorithm previously illustrated, we first store the cumulative sums going down the columns. We then iterate through the indices with our  x1  and  x2  pointers.  So we first get  [0, 1, 0, 0]  as our array, with  x1 = 0  and  x2 = 1 , which we consider  1  to be the largest. We know it is  1x1  in size because ( x2-x1=1 ) and we cover a range of  1 .  We then extend this further, increasing  x2  until we get to  4 , then increment  x1  by one and repeat, resulting in  O(N^2)  time before invoking 1D Kadane's algorithm, which takes  O(N)  time.  When we get  x1=1  and  x2=3 , we have  [0, 2, 2, 1] , which gives us  [2, 2]  with  x2-x1=2 , hence we have our maximum here being a grouping of 4.", 
            "title": "Extending Algorithm to 2D"
        }, 
        {
            "location": "/Dynamic Programming/#storing-results", 
            "text": "We can have several algorithms for storing the results. The easiest way is to store another 2D grid with the maximum pairing so far that encompasses that specific grid cell. If your new candidate is larger and it satisfies, you can set that grid. Else, you can discard it unless another cell requires it to be occupied. This will result in  O(N*M)  memory, which is optimal.", 
            "title": "Storing Results"
        }
    ]
}